---
title:  "[알고리즘] Dynamic programming"
created:   2020-09-14 23:09:11 +0900
updated:   2020-09-14 23:09:11 +0900
author: namu
categories: algorithm
permalink: "/algorithm/:year/:month/:day/:title"
image: https://cdn.pixabay.com/photo/2017/11/12/18/32/book-2943383__480.png
image-view: true
image-author: 200degrees
image-source: https://pixabay.com/ko/users/200degrees-2051452/
---


---

[목차]

1. [Dynamic programming](#dynamic-programming)
2. [피보나치 구현](#피보나치-구현)
3. [분할 정복](#분할-정복)
4. [예제 풀이](#예제-풀이)

---

<br>
## 들어가며

[안경잡이개발자](https://ndb796.tistory.com/)님의
[이것이 취업을 위한 코딩테스트이다 with python 유튭 강의](https://www.youtube.com/watch?v=vRFXpqWDbRU&list=PLRx0vPvlEmdBFBFOoK649FlEMouHISo8N&index=5)에서
발췌했다.

<br>
## Dynamic programming

일반적으로 **_다이나믹dynamic_** 이라는 용어는 런타임 중 임시 데이터를 힙heap 영역에 동적으로 할당하는 것을 의미하지만,
알고리즘에서는 실제 런타임은 아니지만 이미 계산된 값을 별도의 메모리에 캐싱한다는 점에서 관용적?으로 사용된다.

코딩테스트에서 이 알고리즘이 필요한지 바로 캐치하거나 점화식을 생각해내기 어려운 경우가 많으므로
그리디, 완전탐색 등을 빠르게 적용해 보고 문제해결이 되지 않을 때(<del>냄새가 날때</del>) 적용해 본다.

**활용근거**는 다음과 같다.

1. **최적 부분 구조(Optimal Substructure)** 인가?
    - 큰 문제를 작은 문제로 나눌 수 있으며 작은 문제의 답을 모아서 큰 문제 해결
    - 대표적으로 피보나치 수열이 있다(Ni-2 + Ni-1 = Ni)
2. **중복되는 부분 문제(Overlapping Subproblem)** 인가?
    - 동일한 작은 문제를 반복적으로 해결

이것을 **구현하는 방식**에는 다음 두 가지가 있다.

1. **탑다운(Top down)**
    - 큰 문제에서 작은 문제 순으로 재귀적 호출
    - 직관적이고 빠르게 구현 가능하나, stack 메모리를 사용하기에 성능이 상대적으로 느리다
    - 피보나치 기하급수 O(2^n)
2. **보텀업(Bottom up)**
    - 작은 문제에서 큰 문제로 반복 수행
    - 구현을 생각해내기 까다롭지만 상대적으로 성능이 낫다
    - 피보나치 선형 O(N)

<br>
## 피보나치 구현

Fibonacci 는 기하급수적으로 상승하는 O(2^n) 의 대표적인 다이나믹 프로그래밍 예제이다. -> 황금비

| f(1) | f(2) | f(3) | f(4) | f(5) | f(6) | f(7) | f(8) |
|:--|:--|:--|:--|:--|
| 1 | 1 | 2 | 3 | 5 | 8 | 13 | 21 |

**[탑다운]단순 재귀적 구현**: 다이나믹의 특징인 최적 부분 구조에 착안해 탑다운으로 단순 재귀 호출을 한다

```python
# simple recursive
def f(n):
    if n <= 1:
        return n
    return f(n - 1) + f(n - 2)

print(f(4))  # 3
print(f(6))  # 8
print(f(8))  # 21
```

8번째까지의 결과 예상값을 보면 알겠지만, 피보나치에서 모든 순서의 값은 그것의 전과 전전 순서에 대한 연산이 필요하게 된다.
이를 단순 재귀적으로 구현해버리면 최초 입력 값 n 이 증가하는 만큼 동일한 반복연산 횟수도 기하급수적으로 많아진다.

예를 들어 f(6) 을 단순 재귀적으로 호출한다면 **동일한 f(4) 연산이 2회, 동일한 f(3) 연산이 3회, 동일한 f(2) 연산이 5회 반복**된다.

**[탑다운]메모이제이션Memoization**: 위와 같이 동일하게 반복되는 중복연산의 결과값을 메모리에 캐싱Caching 해두는 방법이다

```python
# memoization recursive
caching_list = [0] * 100  # max 100
def memoi_f(n):
    if n <= 1:
        return n
    
    if caching_list[n] != 0:
        return caching_list[n]  # return caching data
    
    caching_list[n] = memoi_f(n - 1) + memoi_f(n - 2)  # caching
    return caching_list[n]
```

결과값 자체는 단순 재귀와 동일하게 기하급수적으로 증가하지만, 메모리 캐싱으로 인해 스택 연산횟수 자체는 딱 n 회이다. -> O(N)

**[보텀업]반복문으로 구현**: 주어진 수 n만큼 아래에서부터 반복적으로 상승시킨다

```python
# loop, using memoization
caching_list = [0] * 100  # max 100
def loop_caching_f(n):
    if n <= 1:
        return n

    caching_list[0], caching_list[1] = 0, 1
    for i in range(2, n + 1):  # 2 to n
        caching_list[i] = caching_list[i - 1] + caching_list[i - 2]
    return caching_list[n]

# loop, using init variable
def loop_init_f(n):
    if n <= 1:
        return n

    result = 0
    v1, v2 = 0, 1
    for i in range(2, n + 1):  # 2 to n
        result = v1 + v2
        v1 = v2
        v2 = result
    return result
```

첫 번째 **loop_caching_f()** 함수는 메모이제이션에서 재귀호출을 반복문으로 변형한 형태이다.
재귀함수가 반복되면 매번 stack 메모리 영역에 누적되므로, 캐싱 리스트가 있는 경우 그것을 단순 반복하는 것이 성능상 더 효율적이다.

두 번째 **loop_init_f()** 함수는 **loop_caching_f()** 함수에서 메모이제이션을 뺀 반복문이다.
여기에서는 원하는 조건에 맞게 초기값만 잘 지정해주면 된다.
정말 보텀업 방식으로 n 크기만큼 단순 반복만 수행하면서도,
**캐싱 리스트를 저장하기 위한 메모리 공간조차 불필요**하기 때문에 때문에 가장 최적의 DP 구현 코드라고 할 수 있다.

<br>
## 분할 정복

앞서 다이나믹 프로그래밍은 ['최적부분구조'와 '중복되는 부분'](#dynamic-programming) 의 특성이 있다는 것을 살펴보았다.

**분할 정복** 알고리즘은 DP 와 같이 최적부분구조(작은 문제들로 구성된 큰 문제)의 특성을 가지는 공통점이 있지만,
중복 부분이 존재하지는 않는다. 작게 반복되는 알고리즘은 존재하는 반면 중복되는 부분 연산이 없기 때문에
캐싱과 같은 기법을 활용할 수 없다는 점을 기억해야 한다. 분할 정복 알고리즘의 대표적인 예가 퀵정렬이다.

**퀵정렬**: 기준 원소(pivot)에 대해 분할된 각 부분에 대해 같은 분할작업을 반복 수행한다.
같은 원소에 대한 분할은 중복수행하지 않으면서 작은 분할이 큰 분할을 구성하는 최적부분구조라 할 수 있다.

```python
# quick sort using Divide and Conquer
def partition(num_list, init, end):
    pivot = num_list[(init + end) // 2]
    while init <= end:
        while num_list[init] < pivot:
            init += 1
        while num_list[end] > pivot:
            end -= 1
        if init <= end:
            num_list[init], num_list[end] = num_list[end], num_list[init]
            init, end = init + 1, end - 1
    return init


def quick_sort(num_list, init, end):
    if end <= init:
        return
    mid = partition(num_list, init, end)  # divide
    quick_sort(num_list, init, mid - 1)
    quick_sort(num_list, mid, end)


def main(num_list):
    quick_sort(num_list, 0, len(num_list) - 1)
    print(num_list)


nums = [9, 3, 10, 7, 8, 2, 5, 1, 6, 4]
main(nums)
```

퀵정렬은 대부분의 프로그래밍 언어의 sort() 함수에서 활용하는 기법이다.
**이상적인 pivot 값이 선택되었을 때 가장 효율적이고 처리속도가 빠르기 때문**이다( -> O(NlogN) ).
하지만 사례 원소 수가 적어 pivot 선택이 한쪽으로 치우치게 되면 효율성이 급격히 떨어질 수도 있다( -> O(N^2) ).
그래서 보통 퀵정렬과 함께 특정 조건 하에 다른 정렬 알고리즘을 활용하도록 구현된다.

<br>
## 예제 풀이

최적 부분 구조를 가지는 위 알고리즘들을 활용한 예제를 살펴보기 위해 안경잡이개발자 강의와
[zerocho.com](https://www.zerocho.com/category/Algorithm/post/584b979a580277001862f182) 을 참조하였다.

**막대기 자르기**: 
